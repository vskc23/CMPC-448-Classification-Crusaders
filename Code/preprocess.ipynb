{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"./News_Category_Dataset_v3.json\"\n",
    "df = pd.read_json(PATH, lines=True)\n",
    "df.dropna(inplace=True)\n",
    "df = df.drop(['link', 'authors','date'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the Headline and associated short description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_review'] = df['headline'] + df['short_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging Similar classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
    "              \"BUSINESS\": \"FINANCE\",\n",
    "              \"MONEY\": \"FINANCE\",\n",
    "              \"PARENTS\": \"PARENTING\",\n",
    "              \"THE WORLDPOST\": \"WORLD NEWS\",\n",
    "              \"WORLDPOST\": \"WORLD NEWS\",\n",
    "              \"STYLE\": \"STYLE & BEAUTY\",\n",
    "              \"GREEN\": \"ENVIRONMENT\",\n",
    "              \"TASTE\": \"FOOD & DRINK\",\n",
    "              \"SCIENCE\": \"SCIENCE & TECH\",\n",
    "              \"TECH\": \"SCIENCE & TECH\",\n",
    "              \"ARTS\": \"ARTS & CULTURE\",\n",
    "              \"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
    "              \"COLLEGE\": \"EDUCATION\",\n",
    "              \"FIFTY\": \"OTHER\",\n",
    "              \"GOOD NEWS\": \"OTHER\",\n",
    "              \"WEDDING\": \"OTHER\",\n",
    "              \"DIVORCE\": \"OTHER\",\n",
    "              \"MEDIA\": \"OTHER\",\n",
    "              \"WEIRD NEWS\": \"OTHER\",\n",
    "                        }, inplace=True\n",
    "            )\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping classes with less than 5000 samples, to reduce class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_categories = df[df['full_review']>5000]['category'].values\n",
    "short_df = df[df['category'].isin(retain_categories)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def text_preprocessing(dataFrame):\n",
    "    \"\"\"\n",
    "    Performing tasks such as tokenization, stop words, numbers, punctuations, and empty strings removal, stemming,\n",
    "    and lemmatization\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, row in dataFrame.iterrows():\n",
    "        tokenized = nltk.word_tokenize(row['full_review'].lower()) # Word Tokenization\n",
    "        punctuation_stopwords_removed = [re.sub('[^\\w\\s]', '', token) for token in tokenized if not token in stop_words] # punctuations and stopwords removal\n",
    "        number_removed = list(map(clean_numbers, punctuation_stopwords_removed)) # numbers removal\n",
    "        contraction_removed = list(map(replace_contractions, number_removed))   # Expanding contractions\n",
    "        head_desc_lemmatized = [lem.lemmatize(token) for token in contraction_removed] # Word Lemmatization\n",
    "        empty_str_removed = [token for token in head_desc_lemmatized if token != ''] # empty strings removal\n",
    "\n",
    "        dataFrame['full_review'][index] = \" \".join(empty_str_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessing(short_df)\n",
    "short_df.to_csv(\"/content/drive/MyDrive/CSE_448/Final/pre_processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "\"POLITICS\": 0,\n",
    "\"WELLNESS\": 1,\n",
    "\"ENTERTAINMENT\": 2,\n",
    "\"PARENTING\": 3,\n",
    "\"STYLE & BEAUTY\" : 4,\n",
    "\"OTHER\": 5,\n",
    "\"TRAVEL\": 6,\n",
    "\"WORLD NEWS\": 7,\n",
    "\"FOOD & DRINK\": 8,\n",
    "\"FINANCE\": 9,\n",
    "\"SPORTS\": 10,\n",
    "\"SCIENCE & TECH\": 11,\n",
    "\"ENVIRONMENT\": 12,\n",
    "\"ARTS & CULTURE\" : 13,\n",
    "\"CRIME\" : 14,\n",
    "\"RELIGION\": 15\n",
    "}\n",
    "\n",
    "label2id = {}\n",
    "for key in id2label.keys():\n",
    "  label2id[id2label[key]] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transformers, converting data to transformer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df['labels'] = short_df['category'].map(id2label)\n",
    "short_df.to_csv(\"/content/drive/MyDrive/CSE_448/Final/transformer_format_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_remove = short_df[short_df['labels'] == 0].sample(26000, random_state=1).index\n",
    "df1 = short_df.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 1].sample(18000, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 2].sample(9500, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 3].sample(6900, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 4].sample(6000, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 5].sample(5000, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 6].sample(4100, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 7].sample(3000, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 8].sample(3000, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "rows_to_remove = df1[df1['labels'] == 9].sample(2000, random_state=1).index\n",
    "df1 = df1.drop(rows_to_remove)\n",
    "\n",
    "df1['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"/content/drive/MyDrive/CSE_448/Final/undersampled_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
